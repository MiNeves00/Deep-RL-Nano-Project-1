{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <p style=\"text-align: center;\">Miguel Carreira Neves</p>\n",
    "<p style=\"text-align: center;\">22/03/2022</p>\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is a proposed solution using Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "For a detailed description of **how to run** this project and all dependencies needed check the `README.md`.\n",
    "\n",
    "For more information and **implementation details** check the `Report.md`. It contains information about the network description, the agent details, interpretation of obtained results and possible future work ideas .\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "import sys\n",
    "print (sys.version) # Python Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between Single or Multi Agent enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Reacher_Linux_single_agent/Reacher.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(n_episodes=2000, max_t=1000, score_obj=30):\n",
    "    \"\"\"DDPG.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        score_obj (int): score objective to stop at when average is reached and save model\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores              \n",
    "    exit = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # reset the environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        agent.next_episode(i_episode)\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            #torch.save(agent, 'agent_checkpoint.pth')\n",
    "            #continueTrain = input('\\Do you want to continue? 0-Yes 1-No')\n",
    "            #if continueTrain == \"1\":\n",
    "                #exit = True\n",
    "        if np.mean(scores_window)>=score_obj or exit is True:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_checkpoint.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_checkpoint.pth')\n",
    "            #torch.save(agent, 'agent_checkpoint.pth')\n",
    "            '''\n",
    "            torch.save({\n",
    "            'i_episode': i_episode,\n",
    "            'actor_state_dict': agent.actor_local.state_dict(),\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'actor_loss': agent.actor_loss,\n",
    "            'critic_state_dict': agent.critic_local,\n",
    "            'critic_optimizer_state_dict': agent.critic_optimizer.state_dict(),\n",
    "            'critic_loss': agent.critic_loss,\n",
    "            'replay_buffer': agent.memory.memory\n",
    "            }, 'agent.pth')\n",
    "            '''\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initiate the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DDPG called with params: state_size=33 action_size=4 random_seed=0 actor_fc1_units=128 actor_fc2_units=128 critic_fc1_units=128 critic_fc2_units=128 buffer_size=100000 batch_size=128 gamma=0.99 tau=0.001 lr_actor=0.0002 lr_critic=0.0005 weight_decay=0 batch_norm=True add_ounoise=-1 mu=0.0 theta=0.15 sigma=0.2 dropout_prob=0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Agent\n",
    "agent = Agent(state_size=state_size,action_size=action_size, random_seed=0, actor_fc1_units=128, actor_fc2_units=128,\n",
    "                critic_fc1_units=128, critic_fc2_units=128,\n",
    "                buffer_size=int(1e5), batch_size=128,\n",
    "                gamma=0.99, tau=1e-3 ,\n",
    "                lr_actor=2e-4, lr_critic=5e-4, weight_decay=0, batch_norm=True, add_ounoise=-1, dropout_prob=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Agent\n",
    "\n",
    "Select what is the score goal.\n",
    "\n",
    "If the model achieves the desired goal it will save itself onto a file and the training will finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the score average over 100 episodes that you want to achieve?\n",
      "(Recommended a score of 30 when training a new network)\n",
      "30\n",
      "Training from the ground up with single agent enviroment...\n",
      "Episode 16\tScore: 1.29"
     ]
    }
   ],
   "source": [
    "score_objective = int(input(\"What is the score average over 100 episodes that you want to achieve?\\n(Recommended a score of 30 when training a new network)\\n\"))\n",
    "print(\"Training from the ground up with single agent enviroment...\")\n",
    "scores = runner(n_episodes=100, score_obj=score_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get scores averages over x episodes\n",
    "averages_num = []\n",
    "averages_100 = []\n",
    "score_sum = 0\n",
    "score_sum_100 = 0\n",
    "i = 1\n",
    "x_episodes = 25\n",
    "for score in scores:\n",
    "    score_sum+=score\n",
    "    score_sum_100+=score\n",
    "    if i%x_episodes == 0:\n",
    "        score_sum = score_sum/x_episodes\n",
    "        for j in range(0,x_episodes):\n",
    "            averages_num.append(score_sum)\n",
    "        if i%100 == 0:\n",
    "            score_sum_100 = score_sum_100/100\n",
    "            for j in range(0,100):\n",
    "                averages_100.append(score_sum_100)\n",
    "            score_sum_100 = 0\n",
    "        score_sum=0\n",
    "    i+=1\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='Score per Episode')\n",
    "plt.plot(np.arange(len(averages_num)), averages_num, label='Avg Score Over ' +str(x_episodes)+ ' Episodes')\n",
    "plt.plot(np.arange(len(averages_100)), averages_100, label='Avg Score Over 100 Episodes')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test and Watch the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the Agent behaves in the enviroment after being trained. Choose if you want the model to be loaded from a file or to use the one just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "#agent = Agent(state_size=state_size, action_size=action_size)\n",
    "fromMemory = input(\"Do you want to test using network from memory? 0-Yes 1-No \")\n",
    "if fromMemory == \"0\":\n",
    "    print(\"Continuing from Memory...\")\n",
    "    #agent = torch.load('agent.pth',map_location=torch.device('cpu'))\n",
    "    agent.actor_local.load_state_dict(torch.load('actor_checkpoint.pth',map_location=torch.device('cpu')))\n",
    "    agent.critic_local.load_state_dict(torch.load('critic_checkpoint.pth',map_location=torch.device('cpu')))\n",
    "else:\n",
    "    print(\"Using the current trained network. If no one has previously been trained agent will act randomly.\")\n",
    "agent.actor_local.eval()\n",
    "agent.critic_local.eval()\n",
    "max_t = 1000\n",
    "scores_test = []\n",
    "\n",
    "test_episodes = 10\n",
    "for episode in range(test_episodes):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]   # get the next state\n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "\n",
    "        state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "    scores_test.append(score)\n",
    "    print(\"Episode: \" + str(episode+1) + \" Score: \" + str(score))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get scores averages over x episodes\n",
    "averages_test = []\n",
    "score_sum_test = 0\n",
    "i = 1\n",
    "test_episodes_avg = 5\n",
    "for score in scores_test:\n",
    "    score_sum_test+=score\n",
    "    if i%test_episodes_avg == 0:\n",
    "        score_sum_test = score_sum_test/test_episodes_avg\n",
    "        for j in range(0,test_episodes_avg):\n",
    "            averages_test.append(score_sum_test)\n",
    "        score_sum_test=0\n",
    "    i+=1\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_test)), scores_test, label='Score per Episode')\n",
    "plt.plot(np.arange(len(averages_test)), averages_test, label='Avg Score Over ' +str(test_episodes_avg)+ ' Episodes')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Avg Score Over All Episodes - ' + str(sum(averages_test)/len(averages_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats you finished experimenting with this Project\n",
    "\n",
    "Read the Report.md for information on Future Work and implementation details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
